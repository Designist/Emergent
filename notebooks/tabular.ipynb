{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular Q-Learning\n",
    "Attribute-value reference game environment (similar to [Kottur, et al.](https://arxiv.org/abs/1706.08502)) with tabular Q-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 30000\n",
    "eta = 0.8\n",
    "gamma = 0.95\n",
    "\n",
    "epsilon = 1.0\n",
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.01\n",
    "decay_rate = 0.0005;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up Q-Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 4\n",
    "num_attributes = 4\n",
    "num_actions = 2\n",
    "num_guesses = 2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A-State: [att * att, vocab * vocab] (+1 for empty vocab)\n",
    "# Q-State: [vocab * vocab, att * att] (+1 for empty attribute)\n",
    "a_table = zeros(num_attributes, num_attributes, vocab_size+1, vocab_size+1, vocab_size)\n",
    "q_table = zeros(vocab_size, vocab_size, num_attributes+1, num_attributes+1, num_attributes)\n",
    "\n",
    "a_visited = falses(num_attributes, num_attributes, vocab_size+1, vocab_size+1)\n",
    "q_visited = falses(vocab_size, vocab_size, num_attributes+1, num_attributes+1)\n",
    "\n",
    "num_correct = 0;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for calculating accuracy:\n",
    "final_a_states = []\n",
    "num_exploits = 0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "groundedness (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Given a list of a_states inputs:\n",
    "# Calculate a measure of groundedness.\n",
    "function groundedness(a_states)\n",
    "    cooccurences = zeros(num_attributes, vocab_size)\n",
    "    for a_state = a_states\n",
    "        cooccurences[a_state[1], a_state[3]] += 1\n",
    "        cooccurences[a_state[1], a_state[4]] += 1\n",
    "        cooccurences[a_state[2], a_state[3]] += 1\n",
    "        cooccurences[a_state[2], a_state[4]] += 1\n",
    "    end\n",
    "    sum = 0\n",
    "    for att in 1:num_attributes\n",
    "        sum += maximum(cooccurences[att, :])\n",
    "    end\n",
    "    sum /= (length(a_states)*num_attributes)\n",
    "    println(\"Groundedness: \", sum)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning\n",
    "Iterate over episodes and update the Q-tables accordingly:\n",
    "\n",
    " * **A-state** - contains randomly selected attributes and previously uttered vocabulary items\n",
    " * **Q-state** - contains heard vocabulary items\n",
    "\n",
    "The goal is for A-bot to communicate its observed attributes to Q-bot. Loosely based on [FreeCodeCamp tutorial](https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Q%20learning/FrozenLake/Q%20Learning%20with%20FrozenLake.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_reward (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function get_reward(a_state, q_state)\n",
    "    if a_state[1:2] == q_state[3:end]\n",
    "        return 1\n",
    "    elseif a_state[1] == q_state[3]\n",
    "        return -0.1\n",
    "    elseif a_state[2] == q_state[4]\n",
    "        return -0.1\n",
    "    else\n",
    "        return -1\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groundedness: 0.3802023608768971\n",
      "Accuracy: 0.9249848229342199\n"
     ]
    }
   ],
   "source": [
    "total_rewards = 0\n",
    "for episode in 1:num_episodes\n",
    "#     println(episode)\n",
    "    # Generate random attributes:\n",
    "    tradeoff = rand() # exploration-exploitation\n",
    "    explore = (tradeoff < epsilon)\n",
    "    \n",
    "    a_state = [rand(1:4), rand(1:4), vocab_size+1, vocab_size+1]\n",
    "    a_states = [copy(a_state)]\n",
    "    # A-Bot:\n",
    "    for action in 1:num_actions\n",
    "        if explore | ~a_visited[a_state...]\n",
    "            # Explore:\n",
    "            word = rand(1:vocab_size)\n",
    "            a_state[action+2] = word\n",
    "        else\n",
    "            # Exploit:\n",
    "#             println(\"Exploiting A-bot...\")\n",
    "            options = a_table[a_state[1], a_state[2], a_state[3], a_state[4], :]\n",
    "#             print(options)\n",
    "            word = argmax(options)\n",
    "            a_state[action+2] = word\n",
    "        end\n",
    "        push!(a_states, copy(a_state))\n",
    "    end\n",
    "        \n",
    "    # Noisy Channel\n",
    "    first_utt = a_state[3]\n",
    "    second_utt = a_state[4]\n",
    "#     if (rand() > 0.9) & (episode < 15000)\n",
    "#         first_utt = rand(1:vocab_size)\n",
    "#     end\n",
    "#     if (rand() > 0.9) & (episode < 15000)\n",
    "#         second_utt = rand(1:vocab_size)\n",
    "#     end\n",
    "    \n",
    "    # Q-Bot:\n",
    "    q_state = [first_utt, second_utt, num_attributes+1, num_attributes+1]\n",
    "    q_states = [copy(q_state)]\n",
    "    for guess in 1:num_guesses\n",
    "        if explore | ~q_visited[q_state...]\n",
    "            # Explore:\n",
    "            att = rand(1:num_attributes)\n",
    "            q_state[guess+2] = att\n",
    "        else\n",
    "            # Exploit:\n",
    "#             println(\"Exploiting Q-bot...\")\n",
    "            options = q_table[q_state[1], q_state[2], q_state[3], q_state[4], :]\n",
    "            att = argmax(options)\n",
    "            q_state[guess+2] = att\n",
    "        end\n",
    "        push!(q_states, copy(q_state))\n",
    "    end\n",
    "    reward = get_reward(a_state, q_state)\n",
    "    for idx in 1:3\n",
    "        state = a_states[idx]\n",
    "        a_visited[state...] = true\n",
    "        if idx < 3\n",
    "            a_table[state[1],state[2],state[3],state[4], a_state[idx+2]] += reward\n",
    "        end\n",
    "    end\n",
    "    for idx in 1:3\n",
    "        state = q_states[idx]\n",
    "        q_visited[state...] = true \n",
    "        if idx < 3\n",
    "            q_table[state[1],state[2],state[3],state[4], q_state[idx+2]] += reward\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*exp(-decay_rate*episode)\n",
    "    if (episode % 100 == 0)\n",
    "#         println(episode, \" | \", epsilon)\n",
    "    end\n",
    "    if (~explore) & (episode > 15000)\n",
    "        push!(final_a_states, copy(a_state))\n",
    "        total_rewards += reward\n",
    "        num_exploits += 1\n",
    "    end\n",
    "    \n",
    "end\n",
    "groundedness(final_a_states)\n",
    "println(\"Accuracy: \", total_rewards/num_exploits)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.0",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
